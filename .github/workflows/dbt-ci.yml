name: dbt CI

on:
  pull_request:
    paths:
      - "dbt_parallel/**"
      - "schema/**"
      - "dags/**"
  push:
    branches: [main]
    paths:
      - "dbt_parallel/**"
      - "schema/**"
      - "dags/**"
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: dbt-ci-${{ github.ref }}
  cancel-in-progress: true

env:
  WAREHOUSE_HOST: localhost
  WAREHOUSE_PORT: 5432
  WAREHOUSE_USER: postgres
  WAREHOUSE_PASSWORD: postgres
  WAREHOUSE_DB: open_data_platform_dw
  DBT_SCHEMA: dbt_ci_${{ github.run_id }}

jobs:
  # ── dbt compile + test (no warehouse needed) ────────────────────
  dbt-lint:
    name: dbt compile & parse
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dbt
        run: |
          python -m pip install --upgrade pip
          python -m pip install dbt-core dbt-postgres

      - name: Install dbt packages
        run: dbt deps --project-dir dbt_parallel --profiles-dir dbt_parallel

      - name: dbt parse (validate SQL + YAML)
        run: dbt parse --project-dir dbt_parallel --profiles-dir dbt_parallel

      - name: dbt compile
        run: dbt compile --project-dir dbt_parallel --profiles-dir dbt_parallel
        continue-on-error: true  # compile may fail without live DB, but parse should not

  # ── dbt build + test (with Postgres service) ────────────────────
  dbt-test:
    name: dbt build & test
    runs-on: ubuntu-latest
    needs: dbt-lint
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: open_data_platform_dw
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U postgres"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install dbt-core dbt-postgres
          python -m pip install -e ".[dev]"

      - name: Install dbt packages
        run: dbt deps --project-dir dbt_parallel --profiles-dir dbt_parallel

      - name: Create source schema and seed tables
        run: |
          PGPASSWORD=postgres psql -h localhost -U postgres -d open_data_platform_dw -c "
            CREATE SCHEMA IF NOT EXISTS job_market_nl;
            CREATE SCHEMA IF NOT EXISTS source_sp1;
            CREATE TABLE IF NOT EXISTS job_market_nl.it_market_snapshot (
              period_key TEXT NOT NULL,
              period_label TEXT,
              sector_name TEXT,
              vacancies DOUBLE PRECISION,
              vacancy_rate DOUBLE PRECISION,
              job_ads_count INTEGER NOT NULL,
              loaded_at TIMESTAMPTZ NOT NULL DEFAULT now()
            );
            CREATE TABLE IF NOT EXISTS job_market_nl.it_market_top_skills (
              skill TEXT NOT NULL,
              count INTEGER NOT NULL,
              loaded_at TIMESTAMPTZ NOT NULL DEFAULT now()
            );
            CREATE TABLE IF NOT EXISTS job_market_nl.it_market_region_distribution (
              region TEXT NOT NULL,
              job_ads_count INTEGER NOT NULL,
              share_pct DOUBLE PRECISION NOT NULL,
              latitude DOUBLE PRECISION NOT NULL,
              longitude DOUBLE PRECISION NOT NULL,
              loaded_at TIMESTAMPTZ NOT NULL DEFAULT now()
            );
            CREATE TABLE IF NOT EXISTS job_market_nl.it_market_job_ads_geo (
              job_id TEXT NOT NULL,
              region TEXT NOT NULL,
              latitude DOUBLE PRECISION NOT NULL,
              longitude DOUBLE PRECISION NOT NULL,
              location_label TEXT,
              loaded_at TIMESTAMPTZ NOT NULL DEFAULT now()
            );

            CREATE TABLE IF NOT EXISTS source_sp1.vacatures (
              vacature_id TEXT PRIMARY KEY,
              title TEXT,
              status TEXT,
              client_name TEXT,
              location TEXT,
              publish_date DATE,
              closing_date DATE,
              hours NUMERIC,
              hours_text TEXT,
              description TEXT,
              category TEXT,
              updated_at_source TIMESTAMP,
              source_url TEXT,
              bronze_object_path TEXT,
              checksum_sha256 TEXT,
              ingested_at TIMESTAMP,
              is_active BOOLEAN
            );

            -- Insert minimal seed data for tests
            INSERT INTO job_market_nl.it_market_snapshot
              (period_key, period_label, sector_name, vacancies, vacancy_rate, job_ads_count)
            VALUES ('2024Q1', '2024 Q1', 'Information and communication', 12000, 3.2, 150);

            INSERT INTO job_market_nl.it_market_top_skills (skill, count)
            VALUES ('Python', 45), ('SQL', 38), ('Kubernetes', 22);

            INSERT INTO job_market_nl.it_market_region_distribution
              (region, job_ads_count, share_pct, latitude, longitude)
            VALUES ('Noord-Holland', 50, 33.3, 52.5279, 4.9500),
                   ('Zuid-Holland', 45, 30.0, 51.9851, 4.4883);

            INSERT INTO job_market_nl.it_market_job_ads_geo
              (job_id, region, latitude, longitude, location_label)
            VALUES ('ad-001', 'Noord-Holland', 52.37, 4.90, 'Amsterdam'),
                   ('ad-002', 'Zuid-Holland', 51.92, 4.48, 'Rotterdam');

            INSERT INTO source_sp1.vacatures (
              vacature_id, title, status, client_name, location, publish_date, closing_date,
              hours, hours_text, description, category, updated_at_source, source_url,
              bronze_object_path, checksum_sha256, ingested_at, is_active
            ) VALUES (
              'vac-001', 'Data Engineer', 'Open', 'FreshMinds', 'Amsterdam',
              CURRENT_DATE - INTERVAL '2 day', CURRENT_DATE + INTERVAL '30 day',
              40, '36-40 uur', 'Build platform pipelines', 'Engineering',
              NOW() - INTERVAL '1 day', 'https://example.com/vac-001',
              'bronze/source_sp1/vac-001.json', 'abc123', NOW(), true
            ) ON CONFLICT (vacature_id) DO NOTHING;
          "

      - name: dbt debug (connection check)
        run: dbt debug --project-dir dbt_parallel --profiles-dir dbt_parallel
        env:
          WAREHOUSE_PORT: "5432"

      - name: dbt build (run + test)
        run: |
          dbt build --project-dir dbt_parallel --profiles-dir dbt_parallel \
            --full-refresh
        env:
          WAREHOUSE_PORT: "5432"

      - name: dbt source freshness
        run: |
          dbt source freshness --project-dir dbt_parallel --profiles-dir dbt_parallel \
            || true  # freshness may warn on CI seed data
        env:
          WAREHOUSE_PORT: "5432"

      - name: dbt docs generate
        run: dbt docs generate --project-dir dbt_parallel --profiles-dir dbt_parallel
        env:
          WAREHOUSE_PORT: "5432"

      - name: Upload dbt artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dbt-artifacts
          path: |
            dbt_parallel/target/manifest.json
            dbt_parallel/target/run_results.json
            dbt_parallel/target/sources.json
            dbt_parallel/target/catalog.json

  # ── Schema & DAG validation ─────────────────────────────────────
  validate:
    name: Schema & DAG validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e ".[dev]"

      - name: Validate DBML schema
        run: make schema-validate

      - name: Validate DAG structure
        run: python scripts/testing/verify_dag_structure.py

      - name: Validate governance metadata
        run: make governance-validate
